{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a multilingual text classifier to identify children's websites\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a multilingual text classifier and evaluating it using cross-validation. Specifically, we fine-tune the `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` model using a dataset containing webpage titles and descriptions in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "Ensure you have the following Python libraries installed:\n",
    "- `torch`\n",
    "- `transformers`\n",
    "- `datasets`\n",
    "- `scikit-learn`\n",
    "- `numpy`\n",
    "- `sentence-transformers`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "For the positive class, which identifies child-directed websites, we utilized sources including the [kidSAFE Seal Program](https://www.kidsafeseal.com/certifiedproducts.html), [CommonSense](https://www.commonsensemedia.org/website-lists) (specifically filtered for content suitable for children below the age of 13), a list compiled by [Kaspersky](https://kids.kaspersky.com/kids-website-list/), and children's websites identified by [VirusTotal](https://www.virustotal.com/gui/home/search) from the top\n",
    "one million websites in the Chrome User Experience Report (CrUX) list from May 2022. For the negative class, which consists of websites not directed at children, we randomly selected webpages from [Common Crawl](https://commoncrawl.org/blog/june-july-2022-crawl-archive-now-available) dataset. To ensure the accuracy and relevance of our dataset, we also conducted a manual review of the samples. Please refer to Section 3 of our paper for more details.\n",
    "\n",
    "\n",
    "The dataset is consists of the following columns:\n",
    "1. **Text**: A string that includes the webpage's title and description concatenated with `\". \"` (dot, space).\n",
    "2. **Label**: A numerical label (either '0.0' or '1.0') indicating the classification category of the text. In this case, '1.0' indicates that the webpage is a child-directed one, while '0.0' indicates that it is not.\n",
    "3. **URL**: The URL of the webpage from which the text is extracted.\n",
    "4. **Language**: The language of the text, indicated by a two-letter code (e.g., 'en' for English)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following constants are used to configure the model training:\n",
    "- `RANDOM_SEED`: Seed for random number generation to ensure reproducibility.\n",
    "- `MODEL_CKPT`: Model checkpoint used for sequence classification.\n",
    "- `BATCH_SIZE`, `LEARNING_RATE`, `EPOCHS`: Training hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "The model is fine-tuned using the following steps:\n",
    "1. Data Loading: Load the data from a pickle file and preprocess it.\n",
    "2. Tokenization: Tokenize text data using a pretrained tokenizer.\n",
    "3. Cross-Validation Setup: Split the data into training and validation sets using KFold cross-validation.\n",
    "4. Training: Train the model on each fold and evaluate its performance.\n",
    "5. Results: Aggregate and display the results from all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "                          AutoTokenizer, TrainingArguments)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Setting up constants and configurations\n",
    "RANDOM_SEED = 200\n",
    "MODEL_CKPT = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "BATCH_SIZE = 12\n",
    "LEARNING_RATE = 4.2e-05\n",
    "EPOCHS = 2\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "train_set_path = 'data/train_data.csv'\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.zeros(1).cuda()\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from a csv file and return a pandas DataFrame.\"\"\"\n",
    "    data = pd.read_csv(file_path, sep=',')\n",
    "    data['labels'] = data['labels'].astype(int)\n",
    "    return data[['text', 'labels', 'urls']]\n",
    "\n",
    "def create_folds(dataframe, n_splits=10):\n",
    "    \"\"\"Create KFold cross-validation folds and return a list of tuples\n",
    "      containing train and validation indices.\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    return [(train_idx, val_idx) for train_idx, val_idx in kf.split(dataframe)]\n",
    "\n",
    "def create_dataset_from_indices(df, indices, tokenizer):\n",
    "    \"\"\"Create a dataset from a dataframe.\"\"\"\n",
    "    dataset = Dataset.from_pandas(df.iloc[indices])\n",
    "    return dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "def tokenize_function(example, tokenizer):\n",
    "    \"\"\"Tokenize a dataset.\"\"\"\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute metrics for a prediction.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {'accuracy': accuracy_score(labels, preds), 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "def train_model_for_fold(train_dataset, val_dataset, tokenizer, model_checkpoint, output_dir):\n",
    "    \"\"\"Train a model for a single fold and return evaluation metrics.\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        logging_steps=len(train_dataset) // BATCH_SIZE,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "        save_total_limit=1       # Optional: Limits the total amount of checkpoints; helps save disk space\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)  # Explicitly save the final model after training\n",
    "    tokenizer.save_pretrained(output_dir)  # Save the tokenizer used with the model\n",
    "    return trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model_with_training_data(train_set_path, model_ckpt, output_dir, n_splits=10):\n",
    "    \"\"\"\n",
    "    Main function to run the cross-validation and train the model.\n",
    "\n",
    "    Parameters:\n",
    "    - train_set_path: str, path to the training dataset file.\n",
    "    - model_ckpt: str, model checkpoint for loading the tokenizer and model.\n",
    "    - output_dir: str, base path for saving output from each fold.\n",
    "    - n_splits: int, number of splits for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - final_results: dict, average evaluation metrics across all folds.\n",
    "    \"\"\"\n",
    "    df_data = load_data(train_set_path)\n",
    "    folds = create_folds(df_data, n_splits=n_splits)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    # check if the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    results = []\n",
    "    for i, (train_idx, val_idx) in enumerate(folds):\n",
    "        output_dir = f\"{output_dir}/output_fold_{i+1}\"  # Use a unique output directory for each fold\n",
    "        train_ds = create_dataset_from_indices(df_data, train_idx, tokenizer)\n",
    "        val_ds = create_dataset_from_indices(df_data, val_idx, tokenizer)\n",
    "\n",
    "        print(f\"Training fold {i+1}\")\n",
    "        result = train_model_for_fold(train_ds, val_ds, tokenizer, model_ckpt, output_dir)\n",
    "        results.append(result)\n",
    "\n",
    "    # Average the evaluation metrics across all folds\n",
    "    final_results = {key: np.mean([result[key] for result in results]) for key in results[0]}\n",
    "    print(\"Average across folds:\", final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_model_with_training_data(train_set_path, MODEL_CKPT, OUTPUT_DIR, n_splits=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
